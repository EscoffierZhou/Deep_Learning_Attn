# Chap0.1 核心数学

## 1.注意力机制的Q/K/V

**Q(Query):当前注意力计算更新位置发出的询问**

>   说是发出询问,比如"图像里还有谁跟我一样，是构成同一条轮廓的边缘？"
>
>   其实都是直接进行相关性分数,然后作为加权系数

**K(Key):当前注意力计算的其它位置(用于被检索的索引),和Q进行相关性计算**

>   因为这里是计算向量相似度,所以这里的"检索的索引"其实是一种高维索引
>
>   也称作内容的语义指纹

**V(Value):序列中每个元素的实际内容,Q-K算完后会往上加权**

## 2.标准注意力机制流程

**从图像的角度理解Attention中的QKV**

>   1.给定输入:**图像的每一个像素点**都是一个向量[R,G,B,X,Y,brightness..]
>
>   2.线性变换:**对每一个像素**的特征向量,使用三个矩阵进行矩阵乘法,得到QKV
>
>   >$Q_i = X_i * W_Q$
>   >
>   >$Q_k = X_i * W_K$
>   >
>   >$Q_v = X_i * W_v$
>
>   3.评分:计算信息检索中的**相关性分数**
>
>   >   $score(A,j) = Q_A \cdot K_j$ (对当前查询位置A的向量和其它所有向量计算)
>   >
>   >   如果点积结果越高,说明Q和K越匹配,即A和其中的j越匹配
>
>   4.归一化
>
>   >   除以根号下K向量维度(保证梯度稳定),然后放入`softmax`
>   >
>   >   ```math
>   >   scaled \ score(A, j) = \frac{score(A, j)}{\sqrt{(d_k)}}
>   >   ```
>   >
>   >   ```math
>   >   attention \ weights_A = softmax([scaled \ score(A,1), ..., scaled \ score(A,n)])
>   >   ```
>
>   5.加权求和(对所有的$V_j$)
>
>   >   $Output_A = α_{A1} * V_1 + α_{A2} * V_2 + ... + α_{An} * V_n$
>   >
>   >   是一个融合了全图所有相关像素信息的、全新的特征向量。

标准自注意力公式$`softmax(\frac{QK^T}{\sqrt(d_k)})`$是置换不变 (permutation-invariant)的。

这意味着如果你打乱输入序列中元素的顺序，但保持每个元素自身不变。

模型本身无法区分“猫在垫子上”和“垫子在猫上”的结构区别

**解决方案：在计算$`QKV`$之前，在原始输入向量中注入位置信息,然后计算$`QKV`$**

>   1.绝对位置编码(原始Transformer):
>
>   使用固定的、非学习的正弦和余弦函数来创建位置向量
>
>   2.相对位置编码(Transformer-XL, T5, DeBERTa):
>
>   不添加位置编码，而在**计算注意力分数时**引入一个额外的可学习的偏差项
>
>   ```math
>   score(i, j) = (Q_i * K_j^T) + R_{i-j}
>   ```

**评分函数 (Scoring Function)：**

>评分函数用于计算 Query 和 Key 之间的相似度或相关性。
>
>**加性注意力 - `Bahdanau Attention`:**
>
>这种方法通过一个带有单个隐藏层的前馈神经网络来计算分数。它在处理 Q 和 K 维度不同的情况时比较灵活。
>
>```math
>score(q, k) = v_a^T \tanh(W_a[q; k])
>```
>
>
>其中 q 是解码器的查询向量，k是编码器的键向量，W_a 和 v_a 是可学习的权重矩阵和向量。[q; k] 表示向量拼接。

>**乘性/点积注意力 - `Luong Attention`:**
>
>这种方法通过点积来计算分数，计算上更高效，特别是当维度较大时可以利用高度优化的矩阵乘法操作。
>
>```math
>\begin{align}
>&score(q, k) = q^T k(基础形式)
>\\ \\
>&score(q, k) = q^T W_a k(通用形式)
>\end{align}
>```

**缩放点积注意力 - `Transformer`:**

>这是 Transformer 中使用的标准注意力形式。
>
>```math
>Attention(Q, K, V) = softmax(\frac{QK^T}{\sqrt{d_k}})V
>```
>
>**“缩放”的必要性:**
>
>这里的$`d_k`$是Key (和 Query) 向量的维度。当$ d_k $变得很大时，$`QK^T `$的点积结果的方差也会随之增大 (方差为$`d_k`$)。
>
>如果点积结果的数值过大，`softmax`函数的梯度会进入其饱和区（即梯度变得非常小，接近于0）。这会导致梯度在反向传播时消失，使得模型训练变得困难或失败。
>
>通过除以$`sqrt(d_k)`$，**可以将点积结果的方差重新调整回 1 左右**，从而避免了梯度消失问题，保证了训练的稳定性。