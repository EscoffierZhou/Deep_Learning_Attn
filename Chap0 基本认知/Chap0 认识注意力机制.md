# Chap0.0 认识注意力



## 1.问题/动机/直觉 

**问题(有关RNN/LSTM/GRU)**在处理长序列时面临两大核心挑战:

**长距离依赖 (Long-Range Dependencies)**

>   RNN 通过一个不断更新的隐藏状态（hidden state）来传递序列信息。
>
>   (理论上，这个隐藏状态可以编码到当前时刻为止的所有信息。)
>
>   但在实践中，由于梯度消失/爆炸问题，信息在序列中传递过长距离后会衰减或失真，导致模型难以捕捉相距较远的词之间的依赖关系。

**信息瓶颈 (Information Bottleneck):** 

>   在经典的 Encoder-Decoder 架构中，编码器 (Encoder) 需要将整个输入序列的所有信息压缩成一个固定长度的上下文向量 (context vector)。
>
>   这个向量成为了解码器 (Decoder) **生成输出的唯一信息来源**。对于长序列或信息量大的序列，这种压缩方式会不可避免地丢失重要细节。

## 2.核心思想

注意力机制的**核心思想**是模仿人类在处理信息时的认知能力：并非平均地处理所有输入，而是**根据当前的任务需求，有选择地“关注”输入中的关键部分，并赋予更高的权重。**

它允许模型在生成每个输出时，都能**直接访问并动态地从输入序列的各个部分提取信息，而不是仅仅依赖于一个压缩后的固定向量。**这样，模型可以根据当前解码的上下文，决定应该“看”向输入的哪个部分。

## 3.对比学习

**与 CNN 的区别与联系**

>   CNN 通过卷积核来捕捉局部模式，其感受野是固定的、局部的。
>
>   注意力机制，特别是自注意力，可以直接计算序列中任意两个位置之间的依赖关系$`O(n^2)`$，无论它们相距多远，实现了全局感受野。
>
>   联系:两者都可以被视为特征提取器(CNN提取局部空间特征，注意力提取基于内容相似度的动态相关性特征)

**与 RNN 的区别与联系**

>   RNN 必须**按时间步顺序处理数据**，这限制了其并行计算能力。
>
>   注意力机制（尤其是 Transformer 中使用的自注意力）**可以同时处理整个序列**，实现了高度并行化，极大地提升了训练效率。

>   RNN 通过**循环结构隐式地捕捉时序关系**
>
>   而自注意力通过**直接计算依赖关系来建模序列**，位置信息需要通过注入额外的位置编码。

>   在注意力机制被引入的早期，它通常与 RNN 结合使用（例如在 `Bahdanau 注意力`中），以增强 RNN Encoder-Decoder 模型的性能。 在这种结构中，注意力帮助解码器在每个时间步关注编码器的不同隐藏状态。