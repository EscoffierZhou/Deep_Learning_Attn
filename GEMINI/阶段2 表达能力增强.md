#### **阶段 2：表达能力增强 (Expressiveness Enhancement)**

这个阶段的目标是让注意力机制能够捕捉更丰富、更精确的模式。

-   
-   **1. 位置信息编码 (Positional Information)**
    -   
    -   **动机：** Transformer 原始架构不包含序列顺序信息，必须手动注入。
    -   **方法：**
        -   
        -   **绝对位置编码 (Absolute):** Transformer 原始论文中的 Sinusoidal 编码。
        -   **相对位置编码 (Relative):** Transformer-XL, T5, DeBERTa 中的相对位置偏置，使模型更关注相对距离。
        -   **旋转位置编码 (Rotary Position Embedding, RoPE):** LLaMA 系列模型使用的技术，通过旋转矩阵将位置信息融入 Q/K 向量，具有优秀的内外插性能。
-   **2. 结构与门控 (Structure and Gating)**
    -   
    -   **目的：** 结合其他结构或引入门控机制来增强非线性和表达力。
    -   **方法：**
        -   
        -   **卷积增强 (Convolution-enhanced):** Conformer, Lightweight/Dynamic Convolution (将 CNN 的局部建模能力与注意力结合)。
        -   **门控机制 (Gating):** Gated Linear Units (GLU), Gated Self-Attention (让模型动态调整信息流)。
        -   **自适应调整 (Adaptation):** Adaptive Attention Span (让模型自主学习需要关注的上下文长度)。