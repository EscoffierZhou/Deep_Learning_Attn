#### **阶段 1：效率优化 (Efficiency Optimization)**

随着模型和序列长度的增加，原始注意力的二次方复杂度成为瓶颈。这个阶段专注于解决计算和内存效率问题。

-   
-   **1. 稀疏注意力 (Sparse Attention): 减少不必要的计算**
    -   
    -   **目的：** 打破全连接计算模式，假设大部分注意力得分是零或可以忽略的。
    -   **代表模型：**
        -   
        -   **固定模式 (Fixed Patterns):** Sparse Transformer, Blockwise Attention, Star-Transformer。
        -   **可学习/自适应模式 (Learnable/Adaptive Patterns):** Routing Transformer。
        -   **结合局部与全局 (Combining Local and Global):** Longformer, BigBird (结合了窗口、全局和随机注意力)。
-   **2. 低秩近似与核方法 (Low-Rank & Kernelized Methods): 改变数学范式**
    -   
    -   **目的：** 将 O(N²) 的矩阵乘法近似为线性复杂度 O(N)。
    -   **代表模型：**
        -   
        -   **低秩分解 (Low-Rank):** Linformer。
        -   **核方法 (Kernelization):** Performer (FAVOR+), Linear Transformer (用核函数近似 Softmax)。
        -   **其他近似：** Nyströmformer, CosFormer (用余弦函数替代 Softmax)。
-   **3. 查询/键简化 (Query/Key Simplification): 减少 K/V 缓存**
    -   
    -   **目的：** 在多头注意力中，让多个查询头共享同一组键和值，大幅减少推理时的显存占用。
    -   **代表模型：**
        -   
        -   **多查询注意力 (Multi-Query Attention, MQA)**。
        -   **分组查询注意力 (Grouped-Query Attention, GQA)** (在 MQA 和 MHA 之间取得平衡)。
-   **4. IO 感知的高效实现 (I/O-Aware Implementation): 硬件层面的极致优化**
    -   
    -   **目的：** 减少 GPU HBM (高带宽内存) 和 SRAM (片上内存) 之间的数据读写次数，这是实际加速的关键。
    -   **核心技术：** **FlashAttention (v1, v2, v3)**，它通过融合、切片 (Tiling) 和重计算 (Recomputation) 技术，在不改变数学公式的前提下实现大幅加速和显存节省，是当前大模型训练和推理的标配。